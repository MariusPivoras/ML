{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f32ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import WeightedRandomSampler, DataLoader\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9602de02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eec7d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed= 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bff0daa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3307/3307 [00:01<00:00, 2600.60it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1102/1102 [00:00<00:00, 2575.46it/s]\n"
     ]
    }
   ],
   "source": [
    "#Creating custom dataset\n",
    "data_path_train = \"C:/Users/M/Desktop/Python/Amber/Data/Train3\"\n",
    "data_path_test = \"C:/Users/M/Desktop/Python/Amber/Data/Test3\"\n",
    "IMG_SIZE = 224\n",
    "\n",
    "transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5), (0.5)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "class CustomDataSet():\n",
    "       \n",
    "    def __init__(self,data_path,transform=None):\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.data = dset.ImageFolder(self.data_path)\n",
    "        self.nrofclass = len(self.data.classes)\n",
    "                \n",
    "        self.custom_data = []\n",
    "        for i in tqdm(range(len(self.data))):\n",
    "            ImgPath = self.data.samples[i][0]\n",
    "            img = cv2.imread(ImgPath)\n",
    "            img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) # gray scale for shape ?\n",
    "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "            Category = self.data.samples[i][1]\n",
    "            y_label = torch.tensor(np.eye(self.nrofclass)[Category])\n",
    "            #y_label = torch.tensor(Category)\n",
    "            self.custom_data.append([img, y_label]) # one hot encoding\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.custom_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, class_id = self.custom_data[idx]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return (img, class_id)\n",
    "\n",
    "DataSetTrain = CustomDataSet(data_path_train,transform=transform)\n",
    "DataSetTest = CustomDataSet(data_path_test,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43423385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking for class balance\n",
    "DataSummary = []\n",
    "class_weights = []\n",
    "for i in range(len(DataSetTrain.data.classes)):\n",
    "    count = DataSetTrain.data.targets.count(i)\n",
    "    DataSummary.append(count)\n",
    "    \n",
    "DataBalance = dict(zip(DataSetTrain.data.classes,DataSummary))\n",
    "\n",
    "Categories = list(DataBalance.keys())\n",
    "Samples = list(DataBalance.values())\n",
    "\n",
    "def addlabels(x,y):\n",
    "    for i in range(len(x)):\n",
    "        plt.text(i, y[i]//2, y[i], ha = 'center', color = 'white')\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.title('Train dataset balance')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Nr of samples')\n",
    "plt.bar(Categories,Samples, color = 'royalblue')\n",
    "addlabels(Categories,Samples)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbd082ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = int(0.2 * len(DataSetTest))\n",
    "test_size = len(DataSetTest) - valid_size\n",
    "valid_dataset, test_dataset = torch.utils.data.random_split(DataSetTest, [valid_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85d2e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise class weights\n",
    "class_weights = []\n",
    "for i in range(len(DataSetTrain.data.classes)):\n",
    "    count = DataSetTrain.data.targets.count(i)\n",
    "    class_weights.append(1/(count))\n",
    "\n",
    "sample_weights = [0] * len(DataSetTrain)\n",
    "\n",
    "for idx, (data, label) in enumerate(DataSetTrain):\n",
    "        class_weight = class_weights[label.argmax()]\n",
    "        sample_weights[idx] = class_weight\n",
    "\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e25d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loaders\n",
    "BATCH_SIZE = 20\n",
    "train_data_loader = DataLoader(DataSetTrain, \n",
    "                               batch_size=BATCH_SIZE,  \n",
    "                               #shuffle=True\n",
    "                               sampler=sampler\n",
    "                               )\n",
    "\n",
    "test_data_loader = DataLoader(test_dataset, \n",
    "                              batch_size=20,  \n",
    "                             # num_workers=0,\n",
    "                              shuffle=False\n",
    "                              )\n",
    "\n",
    "valid_data_loader = DataLoader(valid_dataset, \n",
    "                              batch_size=20,  \n",
    "                             # num_workers=0,\n",
    "                              shuffle=True\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c569f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Vision Transformer (ViT) in PyTorch\n",
    "\n",
    "A PyTorch implement of Vision Transformers as described in:\n",
    "\n",
    "'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale'\n",
    "    - https://arxiv.org/abs/2010.11929\n",
    "\n",
    "`How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers`\n",
    "    - https://arxiv.org/abs/2106.10270\n",
    "\n",
    "The official jax code is released and available at https://github.com/google-research/vision_transformer\n",
    "\n",
    "DeiT model defs and weights from https://github.com/facebookresearch/deit,\n",
    "paper `DeiT: Data-efficient Image Transformers` - https://arxiv.org/abs/2012.12877\n",
    "\n",
    "Acknowledgments:\n",
    "* The paper authors for releasing code and weights, thanks!\n",
    "* I fixed my class token impl based on Phil Wang's https://github.com/lucidrains/vit-pytorch ... check it out\n",
    "for some einops/einsum fun\n",
    "* Simple transformer style inspired by Andrej Karpathy's https://github.com/karpathy/minGPT\n",
    "* Bert reference code checks against Huggingface Transformers and Tensorflow Bert\n",
    "\n",
    "Hacked together by / Copyright 2020, Ross Wightman\n",
    "\"\"\"\n",
    "import math\n",
    "import logging\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
    "from timm.models.helpers import build_model_with_cfg, named_apply, adapt_input_conv\n",
    "from timm.models.layers import PatchEmbed, Mlp, DropPath, trunc_normal_, lecun_normal_\n",
    "from timm.models.registry import register_model\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 6, 'input_size': (1, 224, 224), 'pool_size': None,\n",
    "        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,\n",
    "        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,\n",
    "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = {\n",
    "    # patch models (weights from official Google JAX impl)\n",
    "    'vit_tiny_patch16_224': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/'\n",
    "            'Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224.npz'),\n",
    "    'vit_tiny_patch16_384': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/'\n",
    "            'Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz',\n",
    "        input_size=(3, 384, 384), crop_pct=1.0),\n",
    "    'vit_small_patch32_224': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/'\n",
    "            'S_32-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224.npz'),\n",
    "    'vit_small_patch32_384': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/'\n",
    "            'S_32-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz',\n",
    "        input_size=(3, 384, 384), crop_pct=1.0),\n",
    "    'vit_small_patch16_224': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/'\n",
    "            'S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224.npz'),\n",
    "    'vit_small_patch16_384': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/'\n",
    "            'S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz',\n",
    "        input_size=(3, 384, 384), crop_pct=1.0),\n",
    "    'vit_base_patch32_224': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/'\n",
    "            'B_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224.npz'),\n",
    "    'vit_base_patch32_384': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/'\n",
    "            'B_32-i21k-300ep-lr_0.001-aug_light1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz',\n",
    "        input_size=(3, 384, 384), crop_pct=1.0),\n",
    "    'vit_base_patch16_224': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/'\n",
    "            'B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_224.npz'),\n",
    "    'vit_base_patch16_384': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/'\n",
    "            'B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_384.npz',\n",
    "        input_size=(3, 384, 384), crop_pct=1.0),\n",
    "    'vit_base_patch8_224': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/'\n",
    "            'B_8-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_224.npz'),\n",
    "    'vit_large_patch32_224': _cfg(\n",
    "        url='',  # no official model weights for this combo, only for in21k\n",
    "        ),\n",
    "    'vit_large_patch32_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p32_384-9b920ba8.pth',\n",
    "        input_size=(3, 384, 384), crop_pct=1.0),\n",
    "    'vit_large_patch16_224': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/'\n",
    "            'L_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_224.npz'),\n",
    "    'vit_large_patch16_384': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/'\n",
    "            'L_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_384.npz',\n",
    "        input_size=(3, 384, 384), crop_pct=1.0),\n",
    "\n",
    "    'vit_huge_patch14_224': _cfg(url=''),\n",
    "    'vit_giant_patch14_224': _cfg(url=''),\n",
    "    'vit_gigantic_patch14_224': _cfg(url=''),\n",
    "\n",
    "    # patch models, imagenet21k (weights from official Google JAX impl)\n",
    "    'vit_tiny_patch16_224_in21k': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0.npz',\n",
    "        num_classes=21843),\n",
    "    'vit_small_patch32_224_in21k': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/S_32-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0.npz',\n",
    "        num_classes=21843),\n",
    "    'vit_small_patch16_224_in21k': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0.npz',\n",
    "        num_classes=21843),\n",
    "    'vit_base_patch32_224_in21k': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/B_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.03-do_0.0-sd_0.0.npz',\n",
    "        num_classes=21843),\n",
    "    'vit_base_patch16_224_in21k': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0.npz',\n",
    "        num_classes=21843),\n",
    "    'vit_base_patch8_224_in21k': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/B_8-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0.npz',\n",
    "        num_classes=21843),\n",
    "    'vit_large_patch32_224_in21k': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_patch32_224_in21k-9046d2e7.pth',\n",
    "        num_classes=21843),\n",
    "    'vit_large_patch16_224_in21k': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/L_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1.npz',\n",
    "        num_classes=21843),\n",
    "    'vit_huge_patch14_224_in21k': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/imagenet21k/ViT-H_14.npz',\n",
    "        hf_hub='timm/vit_huge_patch14_224_in21k',\n",
    "        num_classes=21843),\n",
    "\n",
    "    # SAM trained models (https://arxiv.org/abs/2106.01548)\n",
    "    'vit_base_patch32_sam_224': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/sam/ViT-B_32.npz'),\n",
    "    'vit_base_patch16_sam_224': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/sam/ViT-B_16.npz'),\n",
    "\n",
    "    # deit models (FB weights)\n",
    "    'deit_tiny_patch16_224': _cfg(\n",
    "        url='https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth',\n",
    "        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "    'deit_small_patch16_224': _cfg(\n",
    "        url='https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth',\n",
    "        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "    'deit_base_patch16_224': _cfg(\n",
    "        url='https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth',\n",
    "        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "    'deit_base_patch16_384': _cfg(\n",
    "        url='https://dl.fbaipublicfiles.com/deit/deit_base_patch16_384-8de9b5d1.pth',\n",
    "        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, input_size=(3, 384, 384), crop_pct=1.0),\n",
    "    'deit_tiny_distilled_patch16_224': _cfg(\n",
    "        url='https://dl.fbaipublicfiles.com/deit/deit_tiny_distilled_patch16_224-b40b3cf7.pth',\n",
    "        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, classifier=('head', 'head_dist')),\n",
    "    'deit_small_distilled_patch16_224': _cfg(\n",
    "        url='https://dl.fbaipublicfiles.com/deit/deit_small_distilled_patch16_224-649709d9.pth',\n",
    "        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, classifier=('head', 'head_dist')),\n",
    "    'deit_base_distilled_patch16_224': _cfg(\n",
    "        url='https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_224-df68dfff.pth',\n",
    "        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, classifier=('head', 'head_dist')),\n",
    "    'deit_base_distilled_patch16_384': _cfg(\n",
    "        url='https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_384-d0272ac0.pth',\n",
    "        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, input_size=(3, 384, 384), crop_pct=1.0,\n",
    "        classifier=('head', 'head_dist')),\n",
    "\n",
    "    # ViT ImageNet-21K-P pretraining by MILL\n",
    "    'vit_base_patch16_224_miil_in21k': _cfg(\n",
    "        url='https://miil-public-eu.oss-eu-central-1.aliyuncs.com/model-zoo/ImageNet_21K_P/models/timm/vit_base_patch16_224_in21k_miil.pth',\n",
    "        mean=(0, 0, 0), std=(1, 1, 1), crop_pct=0.875, interpolation='bilinear', num_classes=11221,\n",
    "    ),\n",
    "    'vit_base_patch16_224_miil': _cfg(\n",
    "        url='https://miil-public-eu.oss-eu-central-1.aliyuncs.com/model-zoo/ImageNet_21K_P/models/timm'\n",
    "            '/vit_base_patch16_224_1k_miil_84_4.pth',\n",
    "        mean=(0, 0, 0), std=(1, 1, 1), crop_pct=0.875, interpolation='bilinear',\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer\n",
    "\n",
    "    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`\n",
    "        - https://arxiv.org/abs/2010.11929\n",
    "\n",
    "    Includes distillation token & head support for `DeiT: Data-efficient Image Transformers`\n",
    "        - https://arxiv.org/abs/2012.12877\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=True, representation_size=None, distilled=False,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., embed_layer=PatchEmbed, norm_layer=None,\n",
    "                 act_layer=None, weight_init=''):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_chans (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "            distilled (bool): model includes a distillation token and head as in DeiT models\n",
    "            drop_rate (float): dropout rate\n",
    "            attn_drop_rate (float): attention dropout rate\n",
    "            drop_path_rate (float): stochastic depth rate\n",
    "            embed_layer (nn.Module): patch embedding layer\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "            weight_init: (str): weight init scheme\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.num_tokens = 2 if distilled else 1\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "\n",
    "        self.patch_embed = embed_layer(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Representation layer\n",
    "        if representation_size and not distilled:\n",
    "            self.num_features = representation_size\n",
    "            self.pre_logits = nn.Sequential(OrderedDict([\n",
    "                ('fc', nn.Linear(embed_dim, representation_size)),\n",
    "                ('act', nn.Tanh())\n",
    "            ]))\n",
    "        else:\n",
    "            self.pre_logits = nn.Identity()\n",
    "\n",
    "        # Classifier head(s)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        self.head_dist = None\n",
    "        if distilled:\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.init_weights(weight_init)\n",
    "\n",
    "    def init_weights(self, mode=''):\n",
    "        assert mode in ('jax', 'jax_nlhb', 'nlhb', '')\n",
    "        head_bias = -math.log(self.num_classes) if 'nlhb' in mode else 0.\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        if self.dist_token is not None:\n",
    "            trunc_normal_(self.dist_token, std=.02)\n",
    "        if mode.startswith('jax'):\n",
    "            # leave cls token as zeros to match jax impl\n",
    "            named_apply(partial(_init_vit_weights, head_bias=head_bias, jax_impl=True), self)\n",
    "        else:\n",
    "            trunc_normal_(self.cls_token, std=.02)\n",
    "            self.apply(_init_vit_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        # this fn left here for compat with downstream users\n",
    "        _init_vit_weights(m)\n",
    "\n",
    "    @torch.jit.ignore()\n",
    "    def load_pretrained(self, checkpoint_path, prefix=''):\n",
    "        _load_weights(self, checkpoint_path, prefix)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token', 'dist_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        if self.dist_token is None:\n",
    "            return self.head\n",
    "        else:\n",
    "            return self.head, self.head_dist\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        if self.num_tokens == 2:\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        if self.dist_token is None:\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "        else:\n",
    "            x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        if self.dist_token is None:\n",
    "            return self.pre_logits(x[:, 0])\n",
    "        else:\n",
    "            return x[:, 0], x[:, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        if self.head_dist is not None:\n",
    "            x, x_dist = self.head(x[0]), self.head_dist(x[1])  # x must be a tuple\n",
    "            if self.training and not torch.jit.is_scripting():\n",
    "                # during inference, return the average of both classifier predictions\n",
    "                return x, x_dist\n",
    "            else:\n",
    "                return (x + x_dist) / 2\n",
    "        else:\n",
    "            x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _init_vit_weights(module: nn.Module, name: str = '', head_bias: float = 0., jax_impl: bool = False):\n",
    "    \"\"\" ViT weight initialization\n",
    "    * When called without n, head_bias, jax_impl args it will behave exactly the same\n",
    "      as my original init for compatibility with prev hparam / downstream use cases (ie DeiT).\n",
    "    * When called w/ valid n (module name) and jax_impl=True, will (hopefully) match JAX impl\n",
    "    \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        if name.startswith('head'):\n",
    "            nn.init.zeros_(module.weight)\n",
    "            nn.init.constant_(module.bias, head_bias)\n",
    "        elif name.startswith('pre_logits'):\n",
    "            lecun_normal_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "        else:\n",
    "            if jax_impl:\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    if 'mlp' in name:\n",
    "                        nn.init.normal_(module.bias, std=1e-6)\n",
    "                    else:\n",
    "                        nn.init.zeros_(module.bias)\n",
    "            else:\n",
    "                trunc_normal_(module.weight, std=.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    elif jax_impl and isinstance(module, nn.Conv2d):\n",
    "        # NOTE conv was left to pytorch default in my original init\n",
    "        lecun_normal_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm2d)):\n",
    "        nn.init.zeros_(module.bias)\n",
    "        nn.init.ones_(module.weight)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def _load_weights(model: VisionTransformer, checkpoint_path: str, prefix: str = ''):\n",
    "    \"\"\" Load weights from .npz checkpoints for official Google Brain Flax implementation\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    def _n2p(w, t=True):\n",
    "        if w.ndim == 4 and w.shape[0] == w.shape[1] == w.shape[2] == 1:\n",
    "            w = w.flatten()\n",
    "        if t:\n",
    "            if w.ndim == 4:\n",
    "                w = w.transpose([3, 2, 0, 1])\n",
    "            elif w.ndim == 3:\n",
    "                w = w.transpose([2, 0, 1])\n",
    "            elif w.ndim == 2:\n",
    "                w = w.transpose([1, 0])\n",
    "        return torch.from_numpy(w)\n",
    "\n",
    "    w = np.load(checkpoint_path)\n",
    "    if not prefix and 'opt/target/embedding/kernel' in w:\n",
    "        prefix = 'opt/target/'\n",
    "\n",
    "    if hasattr(model.patch_embed, 'backbone'):\n",
    "        # hybrid\n",
    "        backbone = model.patch_embed.backbone\n",
    "        stem_only = not hasattr(backbone, 'stem')\n",
    "        stem = backbone if stem_only else backbone.stem\n",
    "        stem.conv.weight.copy_(adapt_input_conv(stem.conv.weight.shape[1], _n2p(w[f'{prefix}conv_root/kernel'])))\n",
    "        stem.norm.weight.copy_(_n2p(w[f'{prefix}gn_root/scale']))\n",
    "        stem.norm.bias.copy_(_n2p(w[f'{prefix}gn_root/bias']))\n",
    "        if not stem_only:\n",
    "            for i, stage in enumerate(backbone.stages):\n",
    "                for j, block in enumerate(stage.blocks):\n",
    "                    bp = f'{prefix}block{i + 1}/unit{j + 1}/'\n",
    "                    for r in range(3):\n",
    "                        getattr(block, f'conv{r + 1}').weight.copy_(_n2p(w[f'{bp}conv{r + 1}/kernel']))\n",
    "                        getattr(block, f'norm{r + 1}').weight.copy_(_n2p(w[f'{bp}gn{r + 1}/scale']))\n",
    "                        getattr(block, f'norm{r + 1}').bias.copy_(_n2p(w[f'{bp}gn{r + 1}/bias']))\n",
    "                    if block.downsample is not None:\n",
    "                        block.downsample.conv.weight.copy_(_n2p(w[f'{bp}conv_proj/kernel']))\n",
    "                        block.downsample.norm.weight.copy_(_n2p(w[f'{bp}gn_proj/scale']))\n",
    "                        block.downsample.norm.bias.copy_(_n2p(w[f'{bp}gn_proj/bias']))\n",
    "        embed_conv_w = _n2p(w[f'{prefix}embedding/kernel'])\n",
    "    else:\n",
    "        embed_conv_w = adapt_input_conv(\n",
    "            model.patch_embed.proj.weight.shape[1], _n2p(w[f'{prefix}embedding/kernel']))\n",
    "    model.patch_embed.proj.weight.copy_(embed_conv_w)\n",
    "    model.patch_embed.proj.bias.copy_(_n2p(w[f'{prefix}embedding/bias']))\n",
    "    model.cls_token.copy_(_n2p(w[f'{prefix}cls'], t=False))\n",
    "    pos_embed_w = _n2p(w[f'{prefix}Transformer/posembed_input/pos_embedding'], t=False)\n",
    "    if pos_embed_w.shape != model.pos_embed.shape:\n",
    "        pos_embed_w = resize_pos_embed(  # resize pos embedding when different size from pretrained weights\n",
    "            pos_embed_w, model.pos_embed, getattr(model, 'num_tokens', 1), model.patch_embed.grid_size)\n",
    "    model.pos_embed.copy_(pos_embed_w)\n",
    "    model.norm.weight.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/scale']))\n",
    "    model.norm.bias.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/bias']))\n",
    "    if isinstance(model.head, nn.Linear) and model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]:\n",
    "        model.head.weight.copy_(_n2p(w[f'{prefix}head/kernel']))\n",
    "        model.head.bias.copy_(_n2p(w[f'{prefix}head/bias']))\n",
    "    if isinstance(getattr(model.pre_logits, 'fc', None), nn.Linear) and f'{prefix}pre_logits/bias' in w:\n",
    "        model.pre_logits.fc.weight.copy_(_n2p(w[f'{prefix}pre_logits/kernel']))\n",
    "        model.pre_logits.fc.bias.copy_(_n2p(w[f'{prefix}pre_logits/bias']))\n",
    "    for i, block in enumerate(model.blocks.children()):\n",
    "        block_prefix = f'{prefix}Transformer/encoderblock_{i}/'\n",
    "        mha_prefix = block_prefix + 'MultiHeadDotProductAttention_1/'\n",
    "        block.norm1.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/scale']))\n",
    "        block.norm1.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/bias']))\n",
    "        block.attn.qkv.weight.copy_(torch.cat([\n",
    "            _n2p(w[f'{mha_prefix}{n}/kernel'], t=False).flatten(1).T for n in ('query', 'key', 'value')]))\n",
    "        block.attn.qkv.bias.copy_(torch.cat([\n",
    "            _n2p(w[f'{mha_prefix}{n}/bias'], t=False).reshape(-1) for n in ('query', 'key', 'value')]))\n",
    "        block.attn.proj.weight.copy_(_n2p(w[f'{mha_prefix}out/kernel']).flatten(1))\n",
    "        block.attn.proj.bias.copy_(_n2p(w[f'{mha_prefix}out/bias']))\n",
    "        for r in range(2):\n",
    "            getattr(block.mlp, f'fc{r + 1}').weight.copy_(_n2p(w[f'{block_prefix}MlpBlock_3/Dense_{r}/kernel']))\n",
    "            getattr(block.mlp, f'fc{r + 1}').bias.copy_(_n2p(w[f'{block_prefix}MlpBlock_3/Dense_{r}/bias']))\n",
    "        block.norm2.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_2/scale']))\n",
    "        block.norm2.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_2/bias']))\n",
    "\n",
    "\n",
    "def resize_pos_embed(posemb, posemb_new, num_tokens=1, gs_new=()):\n",
    "    # Rescale the grid of position embeddings when loading from state_dict. Adapted from\n",
    "    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n",
    "    _logger.info('Resized position embedding: %s to %s', posemb.shape, posemb_new.shape)\n",
    "    ntok_new = posemb_new.shape[1]\n",
    "    if num_tokens:\n",
    "        posemb_tok, posemb_grid = posemb[:, :num_tokens], posemb[0, num_tokens:]\n",
    "        ntok_new -= num_tokens\n",
    "    else:\n",
    "        posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
    "    gs_old = int(math.sqrt(len(posemb_grid)))\n",
    "    if not len(gs_new):  # backwards compatibility\n",
    "        gs_new = [int(math.sqrt(ntok_new))] * 2\n",
    "    assert len(gs_new) >= 2\n",
    "    _logger.info('Position embedding grid-size from %s to %s', [gs_old, gs_old], gs_new)\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)\n",
    "    posemb_grid = F.interpolate(posemb_grid, size=gs_new, mode='bicubic', align_corners=False)\n",
    "    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_new[0] * gs_new[1], -1)\n",
    "    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n",
    "    return posemb\n",
    "\n",
    "\n",
    "def checkpoint_filter_fn(state_dict, model):\n",
    "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
    "    out_dict = {}\n",
    "    if 'model' in state_dict:\n",
    "        # For deit models\n",
    "        state_dict = state_dict['model']\n",
    "    for k, v in state_dict.items():\n",
    "        if 'patch_embed.proj.weight' in k and len(v.shape) < 4:\n",
    "            # For old models that I trained prior to conv based patchification\n",
    "            O, I, H, W = model.patch_embed.proj.weight.shape\n",
    "            v = v.reshape(O, -1, H, W)\n",
    "        elif k == 'pos_embed' and v.shape != model.pos_embed.shape:\n",
    "            # To resize pos embedding when using model at different size from pretrained weights\n",
    "            v = resize_pos_embed(\n",
    "                v, model.pos_embed, getattr(model, 'num_tokens', 1), model.patch_embed.grid_size)\n",
    "        out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def _create_vision_transformer(variant, pretrained=False, default_cfg=None, **kwargs):\n",
    "    default_cfg = default_cfg or default_cfgs[variant]\n",
    "    if kwargs.get('features_only', None):\n",
    "        raise RuntimeError('features_only not implemented for Vision Transformer models.')\n",
    "\n",
    "    # NOTE this extra code to support handling of repr size for in21k pretrained models\n",
    "    default_num_classes = default_cfg['num_classes']\n",
    "    num_classes = kwargs.get('num_classes', default_num_classes)\n",
    "    repr_size = kwargs.pop('representation_size', None)\n",
    "    if repr_size is not None and num_classes != default_num_classes:\n",
    "        # Remove representation layer if fine-tuning. This may not always be the desired action,\n",
    "        # but I feel better than doing nothing by default for fine-tuning. Perhaps a better interface?\n",
    "        _logger.warning(\"Removing representation layer for fine-tuning.\")\n",
    "        repr_size = None\n",
    "\n",
    "    model = build_model_with_cfg(\n",
    "        VisionTransformer, variant, pretrained,\n",
    "        default_cfg=default_cfg,\n",
    "        representation_size=repr_size,\n",
    "        pretrained_filter_fn=checkpoint_filter_fn,\n",
    "        pretrained_custom_load='npz' in default_cfg['url'],\n",
    "        **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_base_patch16_224(pretrained=False, **kwargs):\n",
    "    \"\"\" ViT-Base (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    \"\"\"\n",
    "    model_kwargs = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, **kwargs)\n",
    "    model = _create_vision_transformer('vit_base_patch16_224', pretrained=pretrained, **model_kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_large_patch16_224(pretrained=False, **kwargs):\n",
    "    \"\"\" ViT-Large model (ViT-L/32) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    \"\"\"\n",
    "    model_kwargs = dict(patch_size=16, embed_dim=1024, depth=24, num_heads=16, **kwargs)\n",
    "    model = _create_vision_transformer('vit_large_patch16_224', pretrained=pretrained, **model_kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_huge_patch14_224(pretrained=False, **kwargs):\n",
    "    \"\"\" ViT-Huge model (ViT-H/14) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    \"\"\"\n",
    "    model_kwargs = dict(patch_size=14, embed_dim=1280, depth=32, num_heads=16, **kwargs)\n",
    "    model = _create_vision_transformer('vit_huge_patch14_224', pretrained=pretrained, **model_kwargs)\n",
    "    return model\n",
    "\n",
    "def vit_giant_patch14_224(pretrained=False, **kwargs):\n",
    "    \"\"\" ViT-Giant model (ViT-g/14) from `Scaling Vision Transformers` - https://arxiv.org/abs/2106.04560\n",
    "    \"\"\"\n",
    "    model_kwargs = dict(patch_size=14, embed_dim=1408, mlp_ratio=48/11, depth=40, num_heads=16, **kwargs)\n",
    "    model = _create_vision_transformer('vit_giant_patch14_224', pretrained=pretrained, **model_kwargs)\n",
    "    return model\n",
    "\n",
    "def vit_small_patch16_224_in21k(pretrained=False, **kwargs):\n",
    "    \"\"\" ViT-Small (ViT-S/16)\n",
    "    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    NOTE: this model has valid 21k classifier head and no representation (pre-logits) layer\n",
    "    \"\"\"\n",
    "    model_kwargs = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6, **kwargs)\n",
    "    model = _create_vision_transformer('vit_small_patch16_224_in21k', pretrained=pretrained, **model_kwargs)\n",
    "    return model\n",
    "\n",
    "def vit_base_patch16_224_in21k(pretrained=False, **kwargs):\n",
    "    \"\"\" ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    NOTE: this model has valid 21k classifier head and no representation (pre-logits) layer\n",
    "    \"\"\"\n",
    "    model_kwargs = dict(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12, **kwargs)\n",
    "    model = _create_vision_transformer('vit_base_patch16_224_in21k', pretrained=pretrained, **model_kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "877d9fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (pre_logits): Identity()\n",
       "  (head): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net= vit_base_patch16_224()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc546688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 166/166 [01:25<00:00,  1.95it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 45/45 [00:07<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Loss: 0.4904704259107163. Validation Loss: 1.7042232752862303\n",
      "Train accuracy: 17.228915662650603. Validation accuracy: 40.333333333333336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 166/166 [01:25<00:00,  1.95it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 45/45 [00:07<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Loss: 0.4657723298131764. Validation Loss: 1.9723811509200566\n",
      "Train accuracy: 17.89156626506024. Validation accuracy: 12.666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 166/166 [01:23<00:00,  1.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 45/45 [00:07<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2. Loss: 0.46356436852272254. Validation Loss: 1.7196027143999482\n",
      "Train accuracy: 16.44578313253012. Validation accuracy: 22.11111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 166/166 [01:24<00:00,  1.97it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 45/45 [00:07<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3. Loss: 0.46203092288872966. Validation Loss: 1.962413221253364\n",
      "Train accuracy: 16.02409638554217. Validation accuracy: 23.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 166/166 [01:24<00:00,  1.97it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 45/45 [00:07<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4. Loss: 0.46006878135224677. Validation Loss: 1.7401053241425841\n",
      "Train accuracy: 14.728915662650602. Validation accuracy: 38.888888888888886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 166/166 [01:23<00:00,  1.99it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 45/45 [00:07<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5. Loss: 0.4593953141908958. Validation Loss: 1.8634513621754716\n",
      "Train accuracy: 16.837349397590362. Validation accuracy: 4.555555555555555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 166/166 [01:25<00:00,  1.95it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 45/45 [00:07<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6. Loss: 0.4565918664552691. Validation Loss: 1.7712413387993975\n",
      "Train accuracy: 16.1144578313253. Validation accuracy: 19.88888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████▎                                | 99/166 [00:51<00:34,  1.95it/s]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "steps = len(train_data_loader)\n",
    "stepsv = len(test_data_loader)\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "train_accs = []\n",
    "for epoch in range(EPOCHS):\n",
    "    total = 0\n",
    "    train_acc = 0\n",
    "    train_loss = 0\n",
    "    net.train()\n",
    "    for step in tqdm(range(steps)):\n",
    "        x, y = next(iter(train_data_loader))\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(x)\n",
    "        loss = loss_function(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        total += y.size(0)\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "  \n",
    "        train_acc += (predicted == y.max(1).indices).sum().cpu().detach().numpy()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    acct=100.*train_acc/total\n",
    "    train_accs.append(acct)\n",
    "    losst= train_loss/len(train_data_loader)\n",
    "    train_losses.append(losst)\n",
    "    \n",
    "    totalv = 0\n",
    "    val_acc = 0\n",
    "    val_loss = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for step in tqdm(range(stepsv)):\n",
    "            vinputs, vlabels = next(iter(valid_data_loader))\n",
    "            vinputs = vinputs.to(device)\n",
    "            vlabels = vlabels.to(device)\n",
    "            voutputs = net(vinputs)\n",
    "            \n",
    "            vloss = loss_function(voutputs, vlabels)\n",
    "            \n",
    "            totalv += vlabels.size(0)\n",
    "        \n",
    "            _, predictedv = voutputs.max(1)\n",
    "            \n",
    "            val_acc += (predictedv == vlabels.max(1).indices).sum().cpu().detach().numpy()\n",
    "            val_loss += vloss.item()\n",
    "            \n",
    "    accuv=100.*val_acc/totalv\n",
    "    val_accs.append(accuv)\n",
    "    lossv= val_loss/len(valid_data_loader)\n",
    "    val_losses.append(lossv)\n",
    "    \n",
    "    print(f\"Epoch: {epoch}. Loss: {losst}. Validation Loss: {lossv}\")\n",
    "    print(f\"Train accuracy: {acct}. Validation accuracy: {accuv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eb0289",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.plot(val_losses,label=\"val\")\n",
    "plt.plot(train_losses,label=\"train\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7fb799",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.plot(val_accs,label=\"val\")\n",
    "plt.plot(train_accs,label=\"train\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9946e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "net.eval()\n",
    "confusion_matrix = torch.zeros(6, 6, dtype=torch.int32)\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(DataSetTest))):\n",
    "        real_class = torch.argmax(DataSetTest[i][1].to('cuda'))\n",
    "        x = DataSetTest[i][0]\n",
    "        x = x.unsqueeze(0)\n",
    "        net_out = net(x.to('cuda'))[0] \n",
    "        predicted_class = torch.argmax(net_out)        \n",
    "        #print (predicted_class.item(),real_class.item())        \n",
    "        if predicted_class == real_class:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        for t, p in zip(real_class.view(-1), predicted_class.view(-1)):\n",
    "                confusion_matrix[t, p] += 1\n",
    "print(\"Accuracy: \", round(correct/total, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f352f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# iterate over test data\n",
    "with torch.no_grad():\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    correct = 0\n",
    "    net.eval()\n",
    "    net.cpu()\n",
    "    total = 0\n",
    "    for inputs, labels in test_data_loader:\n",
    "           \n",
    "            \n",
    "            output = net(inputs) # Feed Network\n",
    "            \n",
    "            pred = output.max(1).indices.to(device)\n",
    "            \n",
    "            true = labels.max(1).indices.to(device)\n",
    "            \n",
    "            output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "            y_pred.extend(output) # Save Prediction\n",
    "            \n",
    "            labels = labels.max(1).indices.cpu().numpy()\n",
    "            y_true.extend(labels) # Save Truth\n",
    "            \n",
    "            total += true.size(0)\n",
    "            correct += torch.sum(pred == true)\n",
    "# constant for classes\n",
    "Classes = ('1', '3', '3', '4', '5','6')\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", (correct/total).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452eba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "from plotcm import plot_confusion_matrix\n",
    "plot_confusion_matrix(cf_matrix, DataSetTrain.data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2926fa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         197,376\n",
      "          Identity-2             [-1, 196, 768]               0\n",
      "        PatchEmbed-3             [-1, 196, 768]               0\n",
      "           Dropout-4             [-1, 197, 768]               0\n",
      "         LayerNorm-5             [-1, 197, 768]           1,536\n",
      "            Linear-6            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-7         [-1, 12, 197, 197]               0\n",
      "            Linear-8             [-1, 197, 768]         590,592\n",
      "           Dropout-9             [-1, 197, 768]               0\n",
      "        Attention-10             [-1, 197, 768]               0\n",
      "         Identity-11             [-1, 197, 768]               0\n",
      "        LayerNorm-12             [-1, 197, 768]           1,536\n",
      "           Linear-13            [-1, 197, 3072]       2,362,368\n",
      "             GELU-14            [-1, 197, 3072]               0\n",
      "          Dropout-15            [-1, 197, 3072]               0\n",
      "           Linear-16             [-1, 197, 768]       2,360,064\n",
      "          Dropout-17             [-1, 197, 768]               0\n",
      "              Mlp-18             [-1, 197, 768]               0\n",
      "         Identity-19             [-1, 197, 768]               0\n",
      "            Block-20             [-1, 197, 768]               0\n",
      "        LayerNorm-21             [-1, 197, 768]           1,536\n",
      "           Linear-22            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-23         [-1, 12, 197, 197]               0\n",
      "           Linear-24             [-1, 197, 768]         590,592\n",
      "          Dropout-25             [-1, 197, 768]               0\n",
      "        Attention-26             [-1, 197, 768]               0\n",
      "         Identity-27             [-1, 197, 768]               0\n",
      "        LayerNorm-28             [-1, 197, 768]           1,536\n",
      "           Linear-29            [-1, 197, 3072]       2,362,368\n",
      "             GELU-30            [-1, 197, 3072]               0\n",
      "          Dropout-31            [-1, 197, 3072]               0\n",
      "           Linear-32             [-1, 197, 768]       2,360,064\n",
      "          Dropout-33             [-1, 197, 768]               0\n",
      "              Mlp-34             [-1, 197, 768]               0\n",
      "         Identity-35             [-1, 197, 768]               0\n",
      "            Block-36             [-1, 197, 768]               0\n",
      "        LayerNorm-37             [-1, 197, 768]           1,536\n",
      "           Linear-38            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-39         [-1, 12, 197, 197]               0\n",
      "           Linear-40             [-1, 197, 768]         590,592\n",
      "          Dropout-41             [-1, 197, 768]               0\n",
      "        Attention-42             [-1, 197, 768]               0\n",
      "         Identity-43             [-1, 197, 768]               0\n",
      "        LayerNorm-44             [-1, 197, 768]           1,536\n",
      "           Linear-45            [-1, 197, 3072]       2,362,368\n",
      "             GELU-46            [-1, 197, 3072]               0\n",
      "          Dropout-47            [-1, 197, 3072]               0\n",
      "           Linear-48             [-1, 197, 768]       2,360,064\n",
      "          Dropout-49             [-1, 197, 768]               0\n",
      "              Mlp-50             [-1, 197, 768]               0\n",
      "         Identity-51             [-1, 197, 768]               0\n",
      "            Block-52             [-1, 197, 768]               0\n",
      "        LayerNorm-53             [-1, 197, 768]           1,536\n",
      "           Linear-54            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-55         [-1, 12, 197, 197]               0\n",
      "           Linear-56             [-1, 197, 768]         590,592\n",
      "          Dropout-57             [-1, 197, 768]               0\n",
      "        Attention-58             [-1, 197, 768]               0\n",
      "         Identity-59             [-1, 197, 768]               0\n",
      "        LayerNorm-60             [-1, 197, 768]           1,536\n",
      "           Linear-61            [-1, 197, 3072]       2,362,368\n",
      "             GELU-62            [-1, 197, 3072]               0\n",
      "          Dropout-63            [-1, 197, 3072]               0\n",
      "           Linear-64             [-1, 197, 768]       2,360,064\n",
      "          Dropout-65             [-1, 197, 768]               0\n",
      "              Mlp-66             [-1, 197, 768]               0\n",
      "         Identity-67             [-1, 197, 768]               0\n",
      "            Block-68             [-1, 197, 768]               0\n",
      "        LayerNorm-69             [-1, 197, 768]           1,536\n",
      "           Linear-70            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-71         [-1, 12, 197, 197]               0\n",
      "           Linear-72             [-1, 197, 768]         590,592\n",
      "          Dropout-73             [-1, 197, 768]               0\n",
      "        Attention-74             [-1, 197, 768]               0\n",
      "         Identity-75             [-1, 197, 768]               0\n",
      "        LayerNorm-76             [-1, 197, 768]           1,536\n",
      "           Linear-77            [-1, 197, 3072]       2,362,368\n",
      "             GELU-78            [-1, 197, 3072]               0\n",
      "          Dropout-79            [-1, 197, 3072]               0\n",
      "           Linear-80             [-1, 197, 768]       2,360,064\n",
      "          Dropout-81             [-1, 197, 768]               0\n",
      "              Mlp-82             [-1, 197, 768]               0\n",
      "         Identity-83             [-1, 197, 768]               0\n",
      "            Block-84             [-1, 197, 768]               0\n",
      "        LayerNorm-85             [-1, 197, 768]           1,536\n",
      "           Linear-86            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-87         [-1, 12, 197, 197]               0\n",
      "           Linear-88             [-1, 197, 768]         590,592\n",
      "          Dropout-89             [-1, 197, 768]               0\n",
      "        Attention-90             [-1, 197, 768]               0\n",
      "         Identity-91             [-1, 197, 768]               0\n",
      "        LayerNorm-92             [-1, 197, 768]           1,536\n",
      "           Linear-93            [-1, 197, 3072]       2,362,368\n",
      "             GELU-94            [-1, 197, 3072]               0\n",
      "          Dropout-95            [-1, 197, 3072]               0\n",
      "           Linear-96             [-1, 197, 768]       2,360,064\n",
      "          Dropout-97             [-1, 197, 768]               0\n",
      "              Mlp-98             [-1, 197, 768]               0\n",
      "         Identity-99             [-1, 197, 768]               0\n",
      "           Block-100             [-1, 197, 768]               0\n",
      "       LayerNorm-101             [-1, 197, 768]           1,536\n",
      "          Linear-102            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-103         [-1, 12, 197, 197]               0\n",
      "          Linear-104             [-1, 197, 768]         590,592\n",
      "         Dropout-105             [-1, 197, 768]               0\n",
      "       Attention-106             [-1, 197, 768]               0\n",
      "        Identity-107             [-1, 197, 768]               0\n",
      "       LayerNorm-108             [-1, 197, 768]           1,536\n",
      "          Linear-109            [-1, 197, 3072]       2,362,368\n",
      "            GELU-110            [-1, 197, 3072]               0\n",
      "         Dropout-111            [-1, 197, 3072]               0\n",
      "          Linear-112             [-1, 197, 768]       2,360,064\n",
      "         Dropout-113             [-1, 197, 768]               0\n",
      "             Mlp-114             [-1, 197, 768]               0\n",
      "        Identity-115             [-1, 197, 768]               0\n",
      "           Block-116             [-1, 197, 768]               0\n",
      "       LayerNorm-117             [-1, 197, 768]           1,536\n",
      "          Linear-118            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-119         [-1, 12, 197, 197]               0\n",
      "          Linear-120             [-1, 197, 768]         590,592\n",
      "         Dropout-121             [-1, 197, 768]               0\n",
      "       Attention-122             [-1, 197, 768]               0\n",
      "        Identity-123             [-1, 197, 768]               0\n",
      "       LayerNorm-124             [-1, 197, 768]           1,536\n",
      "          Linear-125            [-1, 197, 3072]       2,362,368\n",
      "            GELU-126            [-1, 197, 3072]               0\n",
      "         Dropout-127            [-1, 197, 3072]               0\n",
      "          Linear-128             [-1, 197, 768]       2,360,064\n",
      "         Dropout-129             [-1, 197, 768]               0\n",
      "             Mlp-130             [-1, 197, 768]               0\n",
      "        Identity-131             [-1, 197, 768]               0\n",
      "           Block-132             [-1, 197, 768]               0\n",
      "       LayerNorm-133             [-1, 197, 768]           1,536\n",
      "          Linear-134            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-135         [-1, 12, 197, 197]               0\n",
      "          Linear-136             [-1, 197, 768]         590,592\n",
      "         Dropout-137             [-1, 197, 768]               0\n",
      "       Attention-138             [-1, 197, 768]               0\n",
      "        Identity-139             [-1, 197, 768]               0\n",
      "       LayerNorm-140             [-1, 197, 768]           1,536\n",
      "          Linear-141            [-1, 197, 3072]       2,362,368\n",
      "            GELU-142            [-1, 197, 3072]               0\n",
      "         Dropout-143            [-1, 197, 3072]               0\n",
      "          Linear-144             [-1, 197, 768]       2,360,064\n",
      "         Dropout-145             [-1, 197, 768]               0\n",
      "             Mlp-146             [-1, 197, 768]               0\n",
      "        Identity-147             [-1, 197, 768]               0\n",
      "           Block-148             [-1, 197, 768]               0\n",
      "       LayerNorm-149             [-1, 197, 768]           1,536\n",
      "          Linear-150            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-151         [-1, 12, 197, 197]               0\n",
      "          Linear-152             [-1, 197, 768]         590,592\n",
      "         Dropout-153             [-1, 197, 768]               0\n",
      "       Attention-154             [-1, 197, 768]               0\n",
      "        Identity-155             [-1, 197, 768]               0\n",
      "       LayerNorm-156             [-1, 197, 768]           1,536\n",
      "          Linear-157            [-1, 197, 3072]       2,362,368\n",
      "            GELU-158            [-1, 197, 3072]               0\n",
      "         Dropout-159            [-1, 197, 3072]               0\n",
      "          Linear-160             [-1, 197, 768]       2,360,064\n",
      "         Dropout-161             [-1, 197, 768]               0\n",
      "             Mlp-162             [-1, 197, 768]               0\n",
      "        Identity-163             [-1, 197, 768]               0\n",
      "           Block-164             [-1, 197, 768]               0\n",
      "       LayerNorm-165             [-1, 197, 768]           1,536\n",
      "          Linear-166            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-167         [-1, 12, 197, 197]               0\n",
      "          Linear-168             [-1, 197, 768]         590,592\n",
      "         Dropout-169             [-1, 197, 768]               0\n",
      "       Attention-170             [-1, 197, 768]               0\n",
      "        Identity-171             [-1, 197, 768]               0\n",
      "       LayerNorm-172             [-1, 197, 768]           1,536\n",
      "          Linear-173            [-1, 197, 3072]       2,362,368\n",
      "            GELU-174            [-1, 197, 3072]               0\n",
      "         Dropout-175            [-1, 197, 3072]               0\n",
      "          Linear-176             [-1, 197, 768]       2,360,064\n",
      "         Dropout-177             [-1, 197, 768]               0\n",
      "             Mlp-178             [-1, 197, 768]               0\n",
      "        Identity-179             [-1, 197, 768]               0\n",
      "           Block-180             [-1, 197, 768]               0\n",
      "       LayerNorm-181             [-1, 197, 768]           1,536\n",
      "          Linear-182            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-183         [-1, 12, 197, 197]               0\n",
      "          Linear-184             [-1, 197, 768]         590,592\n",
      "         Dropout-185             [-1, 197, 768]               0\n",
      "       Attention-186             [-1, 197, 768]               0\n",
      "        Identity-187             [-1, 197, 768]               0\n",
      "       LayerNorm-188             [-1, 197, 768]           1,536\n",
      "          Linear-189            [-1, 197, 3072]       2,362,368\n",
      "            GELU-190            [-1, 197, 3072]               0\n",
      "         Dropout-191            [-1, 197, 3072]               0\n",
      "          Linear-192             [-1, 197, 768]       2,360,064\n",
      "         Dropout-193             [-1, 197, 768]               0\n",
      "             Mlp-194             [-1, 197, 768]               0\n",
      "        Identity-195             [-1, 197, 768]               0\n",
      "           Block-196             [-1, 197, 768]               0\n",
      "       LayerNorm-197             [-1, 197, 768]           1,536\n",
      "        Identity-198                  [-1, 768]               0\n",
      "          Linear-199                    [-1, 6]           4,614\n",
      "================================================================\n",
      "Total params: 85,257,990\n",
      "Trainable params: 85,257,990\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 408.54\n",
      "Params size (MB): 325.23\n",
      "Estimated Total Size (MB): 733.96\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(net, (1, 224, 224))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
